{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "API.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7Td3tMYc7mE",
        "colab_type": "code",
        "outputId": "cbab3e12-7a27-4b16-cfe9-a9bc134537b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbjKLe4WcJJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mizD-Sqik4rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define data normalizers\n",
        "]min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
        "min_max_scaler_test_stock1 = sklearn.preprocessing.MinMaxScaler()\n",
        "min_max_scaler_test_stock2 = sklearn.preprocessing.MinMaxScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZHI3X8Bk_vH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data(df):\n",
        "    df['Close'] = min_max_scaler.fit_transform(df['Close'].values.reshape(-1,1))\n",
        "    return df\n",
        "\n",
        "# function to create train, validation, test data given stock data and sequence length\n",
        "def load_data(real_data,stock, seq_len,flag):\n",
        "    data_raw = stock.values # convert to numpy array\n",
        "    data = []\n",
        "    data_actual=[]\n",
        "    data_actual_raw = real_data.values\n",
        "    # create all possible sequences of length seq_len\n",
        "    for index in range(len(data_raw) - seq_len): \n",
        "        data.append(data_raw[index: index + seq_len])\n",
        "        data_actual.append(data_actual_raw[index: index + seq_len])\n",
        "    data = np.array(data);\n",
        "    data_actual = np.array(data_actual)\n",
        "    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]));  \n",
        "    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));\n",
        "    train_set_size = data.shape[0] - (valid_set_size + test_set_size);\n",
        "    x_train = data[:train_set_size,:-1,:]\n",
        "    y_train = data_actual[:train_set_size,-1,:]\n",
        "    y_train = min_max_scaler.fit_transform(y_train.reshape(-1,1))\n",
        "    # x_valid = data[train_set_size:train_set_size+valid_set_size,:-1,:]\n",
        "    # y_valid = data_actual[train_set_size:train_set_size+valid_set_size,-1,:]\n",
        "    # y_valid = min_max_scaler.fit_transform(y_valid.reshape(-1,1))\n",
        "    x_test = data[train_set_size:,:-1,:]\n",
        "    y_test = data_actual[train_set_size:,-1,:]\n",
        "    if(flag==1):\n",
        "        y_test = min_max_scaler_test_stock1.fit_transform(y_test.reshape(-1,1))\n",
        "    else:\n",
        "        y_test = min_max_scaler_test_stock2.fit_transform(y_test.reshape(-1,1))\n",
        "    return [x_train, y_train, x_test, y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l03Lv_EQiwKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define high level API that predcits stock value given company and date\n",
        "def get_vals(stock,date):\n",
        "  try:\n",
        "    if(stock=='GM'):\n",
        "      #print(\"GM\")\n",
        "      global GM_dates\n",
        "      global GM_X\n",
        "      global GM_Y\n",
        "      #determine if market was closed or not for that date\n",
        "      if(date in GM_dates):\n",
        "        result = np.where(GM_dates == date)\n",
        "        #print(result)\n",
        "        pred = predict_stock(GM_X[result[0]],stock)\n",
        "        gm_pred = min_max_scaler_test_stock2.inverse_transform(pred[-1])\n",
        "        return(gm_pred[0],GM_Y[result])\n",
        "    else:\n",
        "      global TT_dates\n",
        "      global TT_X\n",
        "      global TT_Y\n",
        "      #determine if market was closed or not for that date\n",
        "      if(date in TT_dates):\n",
        "        result = np.where(TT_dates == date)\n",
        "        pred = predict_stock(TT_X[result[0]],stock)\n",
        "        toyata_pred = min_max_scaler_test_stock1.inverse_transform(pred[-1])\n",
        "        return(toyota_pred[0],TT_Y[result])\n",
        "  except:\n",
        "      return(\"Market CLOSED today\",\"Market CLOSED today\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqX9lUj-pGFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define fucntion to predict stock given input and name of company\n",
        "def predict_stock(X_ip,stock):\n",
        "  graph = tf.Graph()\n",
        "  with graph.as_default():\n",
        "    with tf.Session() as sess:\n",
        "      #restore session variables from pickled files\n",
        "      new_saver = tf.train.import_meta_graph('my_test_model5.meta')\n",
        "      new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
        "      # Input dimension1 = closing price of stock\n",
        "      input_dimensions = 1\n",
        "      batch_size =185\n",
        "      time_size = 20\n",
        "      #Rebuild framework of the model to reload weights into\n",
        "      hidden_size = 256\n",
        "      \n",
        "      '''Though using instances of classes was easily used to build dynamic individual networks while training,\n",
        "       to unpickle the netwrok we need seperate classes to avoid clashing of tensor names while unpickling weights from file\n",
        "      '''\n",
        "      #class for individual network for one stocks\n",
        "      class GRU:\n",
        "          def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
        "              self.input_dimensions = input_dimensions\n",
        "              self.hidden_size = hidden_size\n",
        "              \n",
        "              # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
        "              self.Wr =  graph.get_tensor_by_name('Wr:0')\n",
        "              self.Wz = graph.get_tensor_by_name('Wz:0')\n",
        "              self.Wh = graph.get_tensor_by_name('Wh:0')\n",
        "              \n",
        "              # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
        "              self.Ur = graph.get_tensor_by_name('Ur:0')\n",
        "              self.Uz = graph.get_tensor_by_name('Uz:0')\n",
        "              self.Uh = graph.get_tensor_by_name('Uh:0')\n",
        "              \n",
        "              # Biases for hidden vectors of shape (hidden_size,)\n",
        "              self.br = graph.get_tensor_by_name('br:0')\n",
        "              self.bz = graph.get_tensor_by_name('bz:0')\n",
        "              self.bh = graph.get_tensor_by_name('bh:0')\n",
        "              \n",
        "              # Define the input layer placeholder\n",
        "              self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
        "              \n",
        "              # Put the time-dimension upfront for the scan operator\n",
        "              self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
        "              \n",
        "              # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
        "              self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
        "              \n",
        "              # Perform the scan operator\n",
        "              self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
        "              \n",
        "              # Transpose the result back\n",
        "              self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
        "\n",
        "          def forward_pass(self, h_tm1, x_t):\n",
        "              \"\"\"Perform a forward pass.\n",
        "              \n",
        "              Arguments\n",
        "              ---------\n",
        "              h_tm1: np.matrix\n",
        "                  The hidden state at the previous timestep (h_{t-1}).\n",
        "              x_t: np.matrix\n",
        "                  The input vector.\n",
        "              \"\"\"\n",
        "              # Definitions of z_t and r_t\n",
        "              z_t = tf.sigmoid(tf.matmul(x_t, self.Wz) + tf.matmul(h_tm1, self.Uz) + self.bz)\n",
        "              r_t = tf.sigmoid(tf.matmul(x_t, self.Wr) + tf.matmul(h_tm1, self.Ur) + self.br)\n",
        "              \n",
        "              # Definition of h~_t\n",
        "              h_proposal = tf.tanh(tf.matmul(x_t, self.Wh) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh) + self.bh)\n",
        "              \n",
        "              # Compute the next hidden state\n",
        "              h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
        "              \n",
        "              return h_t\n",
        "          \n",
        "      # Create a new instance of the GRU model\n",
        "      gru = GRU(input_dimensions, hidden_size)\n",
        "      W_output = graph.get_tensor_by_name('W_output:0')\n",
        "      b_output = graph.get_tensor_by_name('b_output:0')\n",
        "      output = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output) + b_output, gru.h_t)\n",
        "      output = tf.transpose(output,[1,0,2])\n",
        "      \n",
        "      #Second class for individual\n",
        "      class GRU1:\n",
        "          def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):\n",
        "              self.input_dimensions = input_dimensions\n",
        "              self.hidden_size = hidden_size\n",
        "              \n",
        "              # Weights for input vectors of shape (input_dimensions, hidden_size)\n",
        "              self.Wr1 =  graph.get_tensor_by_name('Wr1:0')\n",
        "              self.Wz1 = graph.get_tensor_by_name('Wz1:0')\n",
        "              self.Wh1 = graph.get_tensor_by_name('Wh1:0')\n",
        "              \n",
        "              # Weights for hidden vectors of shape (hidden_size, hidden_size)\n",
        "              self.Ur1 = graph.get_tensor_by_name('Ur1:0')\n",
        "              self.Uz1 = graph.get_tensor_by_name('Uz1:0')\n",
        "              self.Uh1 = graph.get_tensor_by_name('Uh1:0')\n",
        "              \n",
        "              # Biases for hidden vectors of shape (hidden_size,)\n",
        "              self.br1 = graph.get_tensor_by_name('br1:0')\n",
        "              self.bz1 = graph.get_tensor_by_name('bz1:0')\n",
        "              self.bh1 = graph.get_tensor_by_name('bh1:0')\n",
        "              \n",
        "              # Define the input layer placeholder\n",
        "              self.input_layer = tf.placeholder(dtype=tf.float64, shape=(None, None, input_dimensions), name='input')\n",
        "              \n",
        "              # Put the time-dimension upfront for the scan operator\n",
        "              self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')\n",
        "              \n",
        "              # A little hack (to obtain the same shape as the input matrix) to define the initial hidden state h_0\n",
        "              self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape=(input_dimensions, hidden_size)), name='h_0')\n",
        "              \n",
        "              # Perform the scan operator\n",
        "              self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=self.h_0, name='h_t_transposed')\n",
        "              \n",
        "              # Transpose the result back\n",
        "              self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')\n",
        "\n",
        "          def forward_pass(self, h_tm1, x_t):\n",
        "              \"\"\"Perform a forward pass.\n",
        "              \n",
        "              Arguments\n",
        "              ---------\n",
        "              h_tm1: np.matrix\n",
        "                  The hidden state at the previous timestep (h_{t-1}).\n",
        "              x_t: np.matrix\n",
        "                  The input vector.\n",
        "              \"\"\"\n",
        "              # Definitions of z_t and r_t\n",
        "              z_t = tf.sigmoid(tf.matmul(x_t, self.Wz1) + tf.matmul(h_tm1, self.Uz1) + self.bz1)\n",
        "              r_t = tf.sigmoid(tf.matmul(x_t, self.Wr1) + tf.matmul(h_tm1, self.Ur1) + self.br1)\n",
        "              \n",
        "              # Definition of h~_t\n",
        "              h_proposal = tf.tanh(tf.matmul(x_t, self.Wh1) + tf.matmul(tf.multiply(r_t, h_tm1), self.Uh1) + self.bh1)\n",
        "              \n",
        "              # Compute the next hidden state\n",
        "              h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)\n",
        "              \n",
        "              return h_t\n",
        "      # Create a new instance of the GRU model\n",
        "      gru1 = GRU1(input_dimensions, hidden_size)   \n",
        "      W_output1 = graph.get_tensor_by_name('W_output1:0')\n",
        "      b_output1 = graph.get_tensor_by_name('b_output1:0')\n",
        "      output1 = tf.map_fn(lambda h_t: tf.matmul(h_t, W_output1) + b_output1, gru1.h_t)\n",
        "      output1 = tf.transpose(output1,[1,0,2]) \n",
        "      if(stock=='GM'):\n",
        "        pred= sess.run([output1],feed_dict={gru1.input_layer:X_ip})\n",
        "      else:\n",
        "        pred = sess.run([output],feed_dict={gru.input_layer:X_ip})\n",
        "      return(pred[-1])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_G88uWrlEBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data\n",
        "df = pd.read_csv('data/toyota_final.csv')\n",
        "dfg = pd.read_csv('data/gm_final.csv')\n",
        "df1 = df[['Close']]\n",
        "dfg1 = dfg[['Close']]\n",
        "#prepare input data for predcition of dates\n",
        "valid_set_size_percentage = 10\n",
        "test_set_size_percentage = 10\n",
        "df_stock_norm = df1.copy()\n",
        "df_stock_norm = normalize_data(df_stock_norm)\n",
        "df_stock_real = df1.copy()\n",
        "# create train, test data\n",
        "seq_len = 20 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = load_data(df_stock_real,df_stock_norm, seq_len,1)\n",
        "dates = df['Date'].values\n",
        "TT_dates = dates[19:-1]\n",
        "TT_X    =  np.concatenate((x_train,x_test), axis=0)\n",
        "TT_Y    = df['Close'].values\n",
        "TT_Y    = TT_Y[19:-1]\n",
        "#normalize data\n",
        "df_stock_norm1 = dfg1.copy()\n",
        "df_stock_norm1 = normalize_data(df_stock_norm1)\n",
        "# create train, test data\n",
        "seq_len = 20 # choose sequence length\n",
        "df_stock_real1 = dfg1.copy()\n",
        "x_train1, y_train1, x_test1, y_test1 = load_data(df_stock_real1,df_stock_norm1, seq_len,0)\n",
        "dates1 = dfg['Date'].values\n",
        "GM_dates = dates1[19:-1]\n",
        "GM_X    =  np.concatenate((x_train1,x_test1), axis=0)\n",
        "GM_Y    = dfg['Close'].values\n",
        "GM_Y    = GM_Y[19:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2tE_u922F1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Enter company GM/TT')\n",
        "comp = input()\n",
        "print('Enter date in format yyyy-mm-dd (between 2012-12-31 to 2019-12-31)')\n",
        "date = input()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbvJ56GsskPZ",
        "colab_type": "code",
        "outputId": "7784189a-8a39-439f-e0af-729517741a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted,expected = get_vals(comp,date)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_test_model5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyL7mstwz0Q2",
        "colab_type": "code",
        "outputId": "6927e743-8898-4721-cb3f-f5b96ad6da7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(predicted,expected)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36.13292719] [37.88999939]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}